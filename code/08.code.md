# Embedding


## What is an Embedding?


## Use Cases: From Modern Search Engines to Self-Driving Cars


### Tesla: How Embeddings Are Used in Self-Driving Cars


### Kalendar AI: The Power of Embedding in Sales Outreach


### Notion: Enhanced Search Capabilities


### DALLÂ·E 2: Text-to-Image Conversion


## Understanding Text Embedding


```python
cat > src/app.py << EOF
from api import client

response = client.embeddings.create(
  model="text-embedding-ada-002",
  input="I am a programmer",
)

print(response)
EOF
```


```python
CreateEmbeddingResponse(
    data=[
        Embedding(
            embedding=[
                -0.016873637, 
                -0.019692589, 
                ..etc
            ], 
            index=0, 
            object='embedding'
        )
    ], 
    model='text-embedding-ada-002', 
    object='list', 
    usage=Usage(
        prompt_tokens=4, 
        total_tokens=4
    )
)            
```


```python
cat > src/app.py << EOF
from api import client

response = client.embeddings.create(
  model="text-embedding-ada-002",
  input="I am a programmer",
)

print(response.data[0].embedding)
EOF
```


## Embeddings for Multiple Inputs


```python
input="I am a programmer",
```


```python
cat > src/app.py << EOF
from api import client

model = "text-embedding-ada-002"
inputs = [
    "I am a programmer", 
    "I am a writer"
    ]
response = client.embeddings.create(
  model=model,
  input=inputs,
)

for i, embedding in enumerate(response.data):
    print(
        f"Embedding for input {i}: "
        f"{embedding.embedding[:5]}..(truncated)"
    )
EOF
```


## Use case: Semantic Search


## Cosine Similarity: A Deeper Look


```python
cat > src/app.py << EOF
import numpy as np
from numpy.linalg import norm

# define two vectors
A = np.array([2,3,5,2,6,7,9,2,3,4])
B = np.array([3,6,3,1,0,9,2,3,4,5])

# print the vectors
print("Vector A: {}".format(A))
print("Vector B: {}".format(B))

# calculate the cosine similarity
cosine = np.dot(A,B)/(norm(A)*norm(B))

# print the cosine similarity
print(
    "Cosine Similarity between A and B: "
    f"{cosine}"
)
EOF
```


```text
Vector A: [2 3 5 2 6 7 9 2 3 4]
Vector B: [3 6 3 1 0 9 2 3 4 5]
Cosine Similarity between A and B: 0.7539959431593041
```


```bash
workon openaigptforpythondevelopers
pip install numpy==1.26.3
```


```python
import numpy as np
from scipy import spatial

# define two vectors
A = np.array([2,3,5,2,6,7,9,2,3,4])
B = np.array([3,6,3,1,0,9,2,3,4,5])

# print the vectors
print(f"Vector A: {A}")
print(f"Vector B: {B}")

# calculate the cosine similarity
cosine = 1 - spatial.distance.cosine(
    A, 
    B
)

# print the cosine similarity
print(f"Cosine Similarity between A and B: {cosine}")
```


```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# define two vectors
A = np.array([2,3,5,2,6,7,9,2,3,4])
B = np.array([3,6,3,1,0,9,2,3,4,5])

# print the vectors
print(f"Vector A: {A}")
print(f"Vector B: {B}")

# calculate the cosine similarity
cosine = cosine_similarity(
    [A],
    [B]
)

# print the cosine similarity
print(f"Cosine Similarity: {cosine[0][0]}")
```


```python
cat > src/app.py << EOF
import numpy as np
from numpy.linalg import norm
from scipy import spatial
from sklearn.metrics.pairwise import cosine_similarity

# define two vectors
A = np.array([2,3,5,2,6,7,9,2,3,4])
B = np.array([3,6,3,1,0,9,2,3,4,5])

# calculate the cosine similarity using NumPy
cosine_np = np.dot(A,B)/(norm(A)*norm(B))

# print the cosine similarity
print(f"Cosine Similarity between A and B: {cosine_np}")


# calculate the cosine similarity using scipy
cosine_sc = 1 - spatial.distance.cosine(A, B)

# print the cosine similarity
print(f"Cosine Similarity between A and B: {cosine_sc}")

# calculate the cosine similarity using sklearn
cosine_sk = cosine_similarity(
    [A],
    [B]
)

# print the cosine similarity
print(f"Cosine Similarity between A and B: {cosine_sk[0][0]}")
EOF
```


```text
Cosine Similarity between A and B: 0.7539959431593041
Cosine Similarity between A and B: 0.7539959431593041
Cosine Similarity between A and B: 0.7539959431593041
```


```bash
workon openaigptforpythondevelopers
pip install scipy scikit-learn
```


## Semantic Search and OpenAI's Text Embeddings


```bash
cat > src/words.csv << EOF
# content of words.csv
text
apple
banana
cherry
dog
cat
house
car
tree
phone
computer
television
book
music
food
water
sky
air
sun
moon
star
ocean
desk
bed
sofa
lamp
carpet
window
door
floor
ceiling
wall
clock
watch
jewelry
ring
necklace
bracelet
earring
wallet
key
photo
EOF
```


```python
df = pd.read_csv('words.csv')
```


```bash
workon openaigptforpythondevelopers
pip install pandas==2.2.0
```

  
```python
cat > src/app.py << EOF
import pandas as pd
df = pd.read_csv('words.csv')
print(df)
EOF
```


```text
          text
0        apple
1       banana
2       cherry
3          dog
4          cat
5        house
6          car
7         tree
8        phone
9     computer
10  television
11        book
12       music
13        food
14       water
15         sky
16         air
17         sun
18        moon
19        star
20       ocean
21        desk
22         bed
23        sofa
24        lamp
25      carpet
26      window
27        door
28       floor
29     ceiling
30        wall
31       clock
32       watch
33     jewelry
34        ring
35    necklace
36    bracelet
37     earring
38      wallet
39         key
40       photo
```


```python
from api import client
import pandas as pd

def get_embedding(text, model):
    text = text.replace("\n", " ")
    return client.embeddings.create(
        input = [text], 
        model=model
    ).data[0].embedding

# create a dataframe from a csv file
df = pd.read_csv('words.csv')
print(df)

# Define the input
input = "The black cat sat on the mat"

# define the model
model = "text-embedding-ada-002"

# get the embedding for the input using 
# the get_embedding() function
embedding = get_embedding(
    input, 
    model=model
    )

print(embedding)
```


```python
# get the embedding for each word in the dataframe
df['embedding'] = df['text'].apply(
    lambda x: get_embedding(
        x, 
        model=model
    )
)
```


```python
df.to_csv('embeddings.csv')
```


```python
cat > src/app.py << EOF
from api import client
import pandas as pd

def get_embedding(text, model):
    text = text.replace("\n", " ")
    return client.embeddings.create(
        input = [text], 
        model=model
    ).data[0].embedding

# create a dataframe from a csv file
df = pd.read_csv('words.csv')

# Define the input text
input = "The black cat sat on the mat"

# define the model
model = "text-embedding-ada-002"

# create a dataframe column for the embeddings
df['embedding'] = df['text'].apply(
    lambda x: get_embedding(
        x, 
        model=model
    )
)

# save the new dataframe to a csv file
df.to_csv('src/embeddings.csv')
EOF
```


```python
df['embedding'] = df['text'].apply(
    lambda x: get_embedding(
        x, 
        model='text-embedding-ada-002'
    )
)
```


```python
df['embedding'] = df['embedding'].apply(
    eval
).apply(
    np.array
)
```


```python
cat > src/app.py << EOF
from api import client
import pandas as pd
import numpy as np

def get_embedding(text, model):
    text = text.replace("\n", " ")
    return client.embeddings.create(
        input = [text], 
        model=model
    ).data[0].embedding

# words.csv is a csv file with a column named 'text' 
# containing words
df = pd.read_csv('words.csv') 

# define the model
model = "text-embedding-ada-002"

# get the embedding for each word in the dataframe
df['embedding'] = df['text'].apply(
    lambda x: get_embedding(
        x, model=model
    )
)

# save the dataframe to a csv file
df.to_csv('embeddings.csv')

# read the csv file
df = pd.read_csv('embeddings.csv')

# convert the embedding axis to a NumPy array
df['embedding'] = df['embedding'].apply(
    eval
).apply(
    np.array
)
EOF
```


```python
# 0) create the python function "cosine_similarity"
def cosine_similarity(a, b):
    numerator = np.dot(a, b)
    denominator = np.linalg.norm(a) * np.linalg.norm(b)
    return numerator / denominator

# 1) get the search term from the user
user_search = input('Enter a search term: ')

# 2) get the embedding for the search term
user_search_embedding = get_embedding(
    user_search, 
    model=model
)

# 3) calculate the cosine similarity between 
# the search term and each word in the dataframe
df['similarity'] = df['embedding'].apply(
    lambda x: cosine_similarity(
        x, 
        user_search_embedding
    )
)
```


```python
cat > src/app.py << EOF
from api import client
import pandas as pd
import numpy as np

def get_embedding(text, model):
    text = text.replace("\n", " ")
    return client.embeddings.create(
        input = [text], 
        model=model
    ).data[0].embedding

def cosine_similarity(a, b):
    numerator = np.dot(a, b)
    denominator = np.linalg.norm(a) * np.linalg.norm(b)
    return numerator / denominator    

# words.csv is a csv file with a column named 'text' 
# containing words
df = pd.read_csv('words.csv') 

# define the model
model = "text-embedding-ada-002"

# get the embedding for each word in the dataframe
df['embedding'] = df['text'].apply(
    lambda x: get_embedding(
        x, 
        model=model
    )
)

# save the dataframe to a csv file
df.to_csv('embeddings.csv')

# read the csv file
df = pd.read_csv('embeddings.csv')

# convert the embedding axis to a NumPy array
df['embedding'] = df['embedding'].apply(
    eval
).apply(
    np.array
)

# get the search term from the user
user_search = input('Enter a search term: ')

# get the embedding for the search term
search_term_embedding = get_embedding(
    user_search, 
    model=model
)

# calculate the cosine similarity between 
# the search term and each word in the dataframe
df['similarity'] = df['embedding'].apply(
    lambda x: cosine_similarity(
        x, 
        search_term_embedding
    )
)

# print the dataframe
print(df)
EOF
```


```python
                text        embedding                                            similarity
0            0       apple  [0.00777884665876627, -0.023069249466061592, -...    0.830240
1            1      banana  [-0.013926557265222073, -0.03288617357611656, ...    0.805654
2            2      cherry  [0.006517840549349785, -0.019012855365872383, ...    0.792175
3            3         dog  [-0.00337603478692472, -0.017694612964987755, ...    0.828722
4            4         cat  [-0.007100660353899002, -0.017430506646633148,...    0.802095
5            5       house  [-0.007176146376878023, 0.007186704780906439, ...    0.874353
6            6         car  [-0.007485327776521444, -0.021592551842331886,...    0.821579
7            7        tree  [-0.00465209037065506, -0.013106998056173325, ...    0.825751
8            8       phone  [-0.0014444905100390315, -0.02283545397222042,...    0.853143
9            9    computer  [-0.003079379675909877, -0.014275545254349709,...    0.861701
10          10  television  [-0.004756690934300423, -0.019938383251428604,...    0.799461
11          11        book  [-0.006820248905569315, -0.01918574422597885, ...    0.838061
12          12       music  [-0.0018608466489240527, -0.023303421214222908...    0.820512
13          13        food  [0.022322265431284904, -0.027532152831554413, ...    0.831643
14          14       water  [0.019045095890760422, -0.012522426433861256, ...    0.816802
15          15         sky  [0.0049745566211640835, -0.0014098514802753925...    0.818561
16          16         air  [0.008967065252363682, -0.023472610861063004, ...    0.801313
17          17         sun  [0.02472955547273159, -0.0024875381495803595, ...    0.816008
18          18        moon  [0.01746830902993679, -0.00917647872120142, 0....    0.801884
19          19        star  [0.011601175181567669, -0.009590022265911102, ...    0.812202
20          20       ocean  [0.0055267661809921265, 3.4062537451973185e-05...    0.798045
21          21        desk  [0.012808148749172688, -0.020803043618798256, ...    0.890042
22          22         bed  [0.005880965385586023, 0.00404005404561758, 0....    0.823849
23          23        sofa  [0.011828833259642124, -0.011590897105634212, ...    0.814421
24          24        lamp  [0.006781177595257759, -0.008739246986806393, ...    0.833910
25          25      carpet  [0.00932883471250534, -0.013073415495455265, -...    0.802651
26          26      window  [0.007422781083732843, -0.016799665987491608, ...    0.829904
27          27        door  [-0.004837467800825834, -0.02687481977045536, ...    0.833266
28          28       floor  [0.018671365454792976, -0.021179255098104477, ...    0.861466
29          29     ceiling  [-0.017032397910952568, -0.009726257994771004,...    0.813452
30          30        wall  [0.0011230265954509377, 0.014665304683148861, ...    0.829238
31          31       clock  [-0.01110783126205206, -0.013732057064771652, ...    0.815311
32          32       watch  [-0.002464910503476858, -0.010481107048690319,...    0.818359
33          33     jewelry  [-0.016047729179263115, 0.01034056767821312, -...    0.783392
34          34        ring  [-0.02057298645377159, -0.02570943906903267, 0...    0.819219
35          35    necklace  [-0.024897022172808647, 0.0024072236847132444,...    0.777741
36          36    bracelet  [-0.03433946892619133, 0.005081846844404936, -...    0.779702
37          37     earring  [-0.025952277705073357, -0.009174846112728119,...    0.776344
38          38      wallet  [0.015422397293150425, -0.020142966881394386, ...    0.825861
39          39         key  [0.0036358926445245743, -0.02881169319152832, ...    0.815508
40          40       photo  [0.004301978275179863, -0.031353630125522614, ...    0.833180

```


```python
# sort the dataframe by the similarity axis
df = df.sort_values(
    by='similarity', 
    ascending=False
)
```


```python
df.head(10)
```


```python
cat > src/app.py << EOF
from api import client
import pandas as pd
import numpy as np

def get_embedding(text, model):
    text = text.replace("\n", " ")
    return client.embeddings.create(
        input = [text], 
        model=model
    ).data[0].embedding

def cosine_similarity(a, b):
    numerator = np.dot(a, b)
    denominator = np.linalg.norm(a) * np.linalg.norm(b)
    return numerator / denominator

# words.csv is a csv file with a column named 'text' 
# containing words
df = pd.read_csv('words.csv')

# define the model
model = "text-embedding-ada-002"

# get the embedding for each word in the dataframe
df['embedding'] = df['text'].apply(
    lambda x: get_embedding(
        x, model=model
    )
)

# save the dataframe to a csv file
df.to_csv('embeddings.csv')

# read the csv file
df = pd.read_csv('embeddings.csv')

# convert the embedding axis to a NumPy array
df['embedding'] = df['embedding'].apply(
    eval
).apply(
    np.array
)

# get the search term from the user
user_search = input('Enter a search term: ')

# get the embedding for the search term
search_term_embedding = get_embedding(
    user_search, 
    model=model
)

# calculate the cosine similarity between 
# the search term and each word in the dataframe
df['similarity'] = df['embedding'].apply(
    lambda x: cosine_similarity(
        x, 
        search_term_embedding
    )
)

# sort the dataframe by the similarity axis
df = df.sort_values(
    by='similarity', 
    ascending=False
)

# print the top 10 similarities
print(df.head(10))
EOF
```


```python
21          21      desk  [0.012808148749172688, -0.020803043618798256, ...    0.890042
5            5     house  [-0.007176146376878023, 0.007186704780906439, ...    0.874353
9            9  computer  [-0.003079379675909877, -0.014275545254349709,...    0.861701
28          28     floor  [0.018671365454792976, -0.021179255098104477, ...    0.861466
8            8     phone  [-0.0014444905100390315, -0.02283545397222042,...    0.853143
11          11      book  [-0.006829570047557354, -0.019116051495075226,...    0.838060
24          24      lamp  [0.006781177595257759, -0.008739246986806393, ...    0.833910
27          27      door  [-0.004837467800825834, -0.02687481977045536, ...    0.833266
40          40     photo  [0.004301978275179863, -0.031353630125522614, ...    0.833180
13          13      food  [0.022320540621876717, -0.026822732761502266, ...    0.831016
```


## Behind the Scenes: How Embeddings Work