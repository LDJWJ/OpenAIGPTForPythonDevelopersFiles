# Autonomous AI-to-AI Discussion Using OpenAI, Weaviate, and AI Avatars


## Generating the Audio Files


```bash
.
├── app
│   ├── app.py
│   ├── Dockerfile.app
│   ├── .env
│   ├── requirements.txt
│   └── static
├── docker-compose.yaml
└── weaviate
    ├── Dockerfile.weaviate
    └── .env
```


```bash
OPENAI_APIKEY=$OPENAI_APIKEY
QUERY_DEFAULTS_LIMIT=25
AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true
PERSISTENCE_DATA_PATH=/var/lib/weaviate
DEFAULT_VECTORIZER_MODULE=text2vec-openai
ENABLE_MODULES=text2vec-openai,generative-openai
CLUSTER_HOSTNAME=node1
```


```python
import weaviate, os
from flask import Flask, session, request
from openai import OpenAI

app = Flask(__name__)
# Set the secret key to some random bytes. 
# Keep this really secret!
app.secret_key = 'CHANGE_THIS_SECRET_KEY'
# The OpenAI API key
openai_api_key = os.getenv("OPENAI_API_KEY")
# The OpenAI model to use for the embeddings
embedding_model = "text-embedding-3-small"
# The class name in Weaviate
weaviate_class_name = "Chat"
# A counter to keep track of the conversation length
counter = 0
# The length of the conversation
conversation_length = 3
# Define the main topic of the conversation
topic = "Joe Biden"
# The initial message from the user to the assistant
initial_prompt = f"Let's talk about this topic {topic}."
# The TTS voices for the user and the assistant
user_voice = "alloy"
assistant_voice = "echo"

openai_client = OpenAI(
    api_key=openai_api_key
)

weaviate_client = weaviate.Client(
    url="http://weaviate:8080",
    auth_client_secret={
        "X-OpenAI-Api-Key": openai_api_key
    }
)
```


```python
schema = {
    "classes": [
        {
            "class": weaviate_class_name,
            "description": "Chat messages",
            "vectorizer": "text2vec-openai",
            "moduleConfig": {
                # Match how OpenAI created the embeddings 
                # for the `content` (`text`) field
                "text2vec-openai": {
                    "model": embedding_model,
                    "type": "text",
                    "vectorizeClassName": False,
                    "VectorizePropertyName": False
                },
                "generative-openai": {
                    "model": "gpt-4",                    
                    "temperatureProperty": 1.2,
                    "maxTokensProperty": 100,
                    "frequencyPenaltyProperty": 2.0,
                    "presencePenaltyProperty": 2.0,
                }                
            },            
            "properties": [

                {
                    "name": "role",
                    "description": "The role",
                    "dataType": ["string"],
                    # Don't vectorize the role
                    "moduleConfig": {
                        "text2vec-openai": {
                            "skip": True
                        }
                    }
                },                
                {
                    "name": "content",
                    "description": "The content",
                    "dataType": ["text"],
                    "moduleConfig": {
                        "text2vec-openai": {
                            "skip": False,                            
                        }
                    },                    
                },
                {
                    "name": "index",
                    "dataType": ["int"],
                    "description": "The index",
                    "moduleConfig": {
                        "text2vec-openai": {
                            "skip": True
                        }
                    }
                }
            ],
        }
    ]
}
```


```python
@app.route("/chat", methods=["GET"])
def assistant_to_user():
    global counter    
    role = request.args.get("role")
    user_session_key = "user_answer" 
    assistant_session_key = "assistant_answer"

  

    if role == "user":
        # Get the previous message from the 2nd character      
        previous_message = session.get(
            assistant_session_key,
            initial_prompt
        )     
        # Define the personality of the 1st character
        personality ="""You are Max, a cheerful humorist, 
        a master of light-hearted puns and dad jokes with 
        a witty twist. Think of Max as a sunny friend who 
        finds joy in every situation 
        and shares it through family-friendly humor. 
        Your goal is to uplift and entertain, 
        making everyone feel welcomed with a kind-hearted 
        laugh.        
        """
        # Define the voice of the 1st character
        voice = user_voice
        # Define the session key of the 1st character
        my_session_key = user_session_key
        # Define the role of the 1st character
        interlocutor_role = "assistant"
        
    elif role == "assistant":
        # Get the previous message from the 1st character
        previous_message = session.get(
            user_session_key,
            initial_prompt
        )
        # Define the personality of the 2nd character
        personality ="""You are Zoe. 
        You offer a quirky and edgy sense of humor, 
        blending clever quips with a dash of sarcasm. 
        You are the adventurous spirit who loves 
        wordplay and the absurd, often surprising you 
        with a clever twist. Your goal is to entertain 
        and engage, making everyone feel included with 
        a clever, unexpected laugh.
        """
        # Define the voice of the 2nd character
        voice = assistant_voice
        # Define the session key of the 2nd character
        my_session_key = assistant_session_key
        # Define the role of the 2nd character
        interlocutor_role = "user"
    else:
        # Return an error if the role is invalid
        # Role can only be "user" or "assistant"
        return {
            "error": "Invalid role"
        }, 400     

    # Define a shared prompt for both characters
    base_prompt = """
    You are in the middle of an ongoing conversation.
    No need to say "hi" or "hello". 
    Then, ask a question or share something interesting.
    Don't be boring.         
    Stick as much as possible to the topic.
    Topic can change but not too abruptly.
    When it changes, it should be a smooth transition 
    and always related to the main topic:""" +  topic \
    + """

    Following text describes the current context:

    {content}
    

    This is the last message from the user: 
    
    """ + previous_message + """
    
    If the counnter is greater than"""  + \
    str(conversation_length)  + """.
    Both you and the user should end the conversation.
    The counter is now: """ + str(counter)

    # Add the personality to the prompt
    prompt = personality + "\n" + base_prompt

    # Save previous_message from the interlocutor 
    # to Weaviate before generating the response
    if counter == 0:
        # Initial data including the initial prompt
        interlocutor_data = {
            "role": "user",
            "content": initial_prompt,
            "index": counter,
        }
    else:
        # Subsequent data
        interlocutor_data = {
            "role": interlocutor_role,
            "content": previous_message,
            "index": counter
        }

    # Save to weaviate function
    weaviate_save_data(
        interlocutor_data["role"], 
        interlocutor_data["content"], 
        interlocutor_data["index"]
    )
    counter += 1  

    try:        
        full_result = generate_response(
            prompt, 
            previous_message
        )        
        result_text = full_result['data']['Get']\
                 [weaviate_class_name][0]\
                 ['_additional']['generate']\
                 ['groupedResult']
        response_code = 200
    except Exception as e:
        app.logger.error(
            f"An error occurred while getting data: {e}"
        )
        full_result = "Error"
        result_text = "Can you please repeat?"        
        response_code = 500
                        
    # save result_text to the session
    session[my_session_key] = result_text

    # text to speech using TTS models
    destination = os.path.join(
        "static/", 
        str(counter) + ".mp3"
    )
    text_to_speech(
        result_text, 
        voice,
        destination
    )
    

    return {
        "result_text": result_text,
        "result_audio": destination,
        "full_result": full_result,
        "interlocutor_data": interlocutor_data,     

    }, response_code
```


```python
def weaviate_save_data(
        role, 
        content, 
        index
    ):
    properties = {
        "role": role,
        "content": content,
        "index": index
    }
    weaviate_client.batch.configure(
        batch_size=10
    )
    with weaviate_client.batch as batch:
        batch.add_data_object(
            properties, 
            class_name=weaviate_class_name,
        )

def generate_response(
        prompt, 
        previous_message
    ):
    nearText = {
        "concepts": [previous_message],
    }

    properties = [
        "role",
        "content",
        "index",
        "_additional {distance}"
    ]

    limit = 5

    generative = weaviate_client.query.get(
        class_name=weaviate_class_name,
        properties=properties,
    ).with_generate(
        grouped_task=prompt
    ).with_near_text(
        nearText
    ).with_limit(
        limit
    ).do()

    return generative

def text_to_speech(
        text, 
        voice, 
        destination, 
        model="tts-1"
    ):
    response = openai_client.audio.speech.create(
        model=model,
        voice=voice,
        input=text
    )

    response.stream_to_file(destination)
```


```python
@app.route("/init", methods=["GET"])
def init():
    global counter
    # clear the session
    session.clear()
    weaviate_delete_data()
    weaviate_create_schema()

    destination = os.path.join(
        "static/" 
        + str(counter) + ".mp3"
    )
    text_to_speech(
        initial_prompt,
        user_voice,
        destination
    )                 
    return {
        "status": "initialized"
    }, 200
```


```python
def weaviate_create_schema():
    try:
        # Create the schema
        weaviate_client.schema.create(schema)
        app.logger.debug(
            "Schema successfully created."
        )
    except Exception as e:
        app.logger.error(
            f"Failed to create schema: {e}"
        )

def weaviate_delete_data():
    try:
        # Delete the data
        weaviate_client.schema.delete_class(
            class_name=weaviate_class_name
        )
        app.logger.debug("Data successfully reset.")
    except Exception as e:
        app.logger.error(
            f"An error occurred while deleting class: {e}"
        )
        return {
            "error in weaviate_reset": str(e)
        }, 500
```


```python
cat << 'EOT' > src/vector_db_generative_search/app/app.py
import weaviate, os
from flask import Flask, session, request
from openai import OpenAI

app = Flask(__name__)
# Set the secret key to some random bytes. 
# Keep this really secret!
app.secret_key = 'CHANGE_THIS_SECRET_KEY'
# The OpenAI API key
openai_api_key = os.getenv("OPENAI_API_KEY")
# The OpenAI model to use for the embeddings
embedding_model = "text-embedding-3-small"
# The class name in Weaviate
weaviate_class_name = "Chat"
# A counter to keep track of the conversation length
counter = 0
# The length of the conversation
conversation_length = 3
# Define the main topic of the conversation
topic = "Joe Biden"
# The initial message from the user to the assistant
initial_prompt = f"Let's talk about this topic {topic}."
# The TTS voices for the user and the assistant
user_voice = "alloy"
assistant_voice = "echo"

openai_client = OpenAI(
    api_key=openai_api_key
)

weaviate_client = weaviate.Client(
    url="http://weaviate:8080",
    auth_client_secret={
        "X-OpenAI-Api-Key": openai_api_key
    }
)

schema = {
    "classes": [
        {
            "class": weaviate_class_name,
            "description": "Chat messages",
            "vectorizer": "text2vec-openai",
            "moduleConfig": {
                # Match how OpenAI created the embeddings 
                # for the `content` (`text`) field
                "text2vec-openai": {
                    "model": embedding_model,
                    "type": "text",
                    "vectorizeClassName": False,
                    "VectorizePropertyName": False
                },
                "generative-openai": {
                    "model": "gpt-4",                    
                    "temperatureProperty": 1.2,
                    "maxTokensProperty": 100,
                    "frequencyPenaltyProperty": 2.0,
                    "presencePenaltyProperty": 2.0,
                }                
            },            
            "properties": [

                {
                    "name": "role",
                    "description": "The role",
                    "dataType": ["string"],
                    # Don't vectorize the role
                    "moduleConfig": {
                        "text2vec-openai": {
                            "skip": True
                        }
                    }
                },                
                {
                    "name": "content",
                    "description": "The content",
                    "dataType": ["text"],
                    "moduleConfig": {
                        "text2vec-openai": {
                            "skip": False,                            
                        }
                    },                    
                },
                {
                    "name": "index",
                    "dataType": ["int"],
                    "description": "The index",
                    "moduleConfig": {
                        "text2vec-openai": {
                            "skip": True
                        }
                    }
                }
            ],
        }
    ]
}

def weaviate_create_schema():
    try:
        # Create the schema
        weaviate_client.schema.create(schema)
        app.logger.debug(
            "Schema successfully created."
        )
    except Exception as e:
        app.logger.error(
            f"Failed to create schema: {e}"
        )

def weaviate_delete_data():
    try:
        # Delete the data
        weaviate_client.schema.delete_class(
            class_name=weaviate_class_name
        )
        app.logger.debug("Data successfully reset.")
    except Exception as e:
        app.logger.error(
            f"An error occurred while deleting class: {e}"
        )
        return {
            "error in weaviate_reset": str(e)
        }, 500
    
def weaviate_save_data(
        role, 
        content, 
        index
    ):
    properties = {
        "role": role,
        "content": content,
        "index": index
    }
    weaviate_client.batch.configure(
        batch_size=10
    )
    with weaviate_client.batch as batch:
        batch.add_data_object(
            properties, 
            class_name=weaviate_class_name,
        )

def generate_response(
        prompt, 
        previous_message
    ):
    nearText = {
        "concepts": [previous_message],
    }

    properties = [
        "role",
        "content",
        "index",
        "_additional {distance}"
    ]

    limit = 5

    generative = weaviate_client.query.get(
        class_name=weaviate_class_name,
        properties=properties,
    ).with_generate(
        grouped_task=prompt
    ).with_near_text(
        nearText
    ).with_limit(
        limit
    ).do()

    return generative

def text_to_speech(
        text, 
        voice, 
        destination, 
        model="tts-1"
    ):
    response = openai_client.audio.speech.create(
        model=model,
        voice=voice,
        input=text
    )

    response.stream_to_file(destination)

@app.route("/init", methods=["GET"])
def init():
    global counter
    # clear the session
    session.clear()
    weaviate_delete_data()
    weaviate_create_schema()

    destination = os.path.join(
        "static/" 
        + str(counter) + ".mp3"
    )
    text_to_speech(
        initial_prompt,
        user_voice,
        destination
    )                 
    return {
        "status": "initialized"
    }, 200

@app.route("/chat", methods=["GET"])
def assistant_to_user():
    global counter    
    role = request.args.get("role")
    user_session_key = "user_answer" 
    assistant_session_key = "assistant_answer"

  

    if role == "user":
        # Get the previous message from the 2nd character      
        previous_message = session.get(
            assistant_session_key,
            initial_prompt
        )     
        # Define the personality of the 1st character
        personality ="""You are Max, a cheerful humorist, 
        a master of light-hearted puns and dad jokes with 
        a witty twist. Think of Max as a sunny friend who 
        finds joy in every situation 
        and shares it through family-friendly humor. 
        Your goal is to uplift and entertain, 
        making everyone feel welcomed with a kind-hearted 
        laugh.        
        """
        # Define the voice of the 1st character
        voice = user_voice
        # Define the session key of the 1st character
        my_session_key = user_session_key
        # Define the role of the 1st character
        interlocutor_role = "assistant"
        
    elif role == "assistant":
        # Get the previous message from the 1st character
        previous_message = session.get(
            user_session_key,
            initial_prompt
        )
        # Define the personality of the 2nd character
        personality ="""You are Zoe. 
        You offer a quirky and edgy sense of humor, 
        blending clever quips with a dash of sarcasm. 
        You are the adventurous spirit who loves 
        wordplay and the absurd, often surprising you 
        with a clever twist. Your goal is to entertain 
        and engage, making everyone feel included with 
        a clever, unexpected laugh.
        """
        # Define the voice of the 2nd character
        voice = assistant_voice
        # Define the session key of the 2nd character
        my_session_key = assistant_session_key
        # Define the role of the 2nd character
        interlocutor_role = "user"
    else:
        # Return an error if the role is invalid
        # Role can only be "user" or "assistant"
        return {
            "error": "Invalid role"
        }, 400     

    # Define a shared prompt for both characters
    base_prompt = """
    You are in the middle of an ongoing conversation.
    No need to say "hi" or "hello". 
    Then, ask a question or share something interesting.
    Don't be boring.         
    Stick as much as possible to the topic.
    Topic can change but not too abruptly.
    When it changes, it should be a smooth transition 
    and always related to the main topic:""" +  topic \
    + """

    Following text describes the current context:

    {content}
    

    This is the last message from the user: 
    
    """ + previous_message + """
    
    If the counnter is greater than"""  + \
    str(conversation_length)  + """.
    Both you and the user should end the conversation.
    The counter is now: """ + str(counter)

    # Add the personality to the prompt
    prompt = personality + "\n" + base_prompt

    # Save previous_message from the interlocutor 
    # to Weaviate before generating the response
    if counter == 0:
        # Initial data
        interlocutor_data = {
            "role": "user",
            "content": initial_prompt,
            "index": counter,
        }
    else:
        # Subsequent data
        interlocutor_data = {
            "role": interlocutor_role,
            "content": previous_message,
            "index": counter
        }

    # Save to weaviate function
    weaviate_save_data(
        interlocutor_data["role"], 
        interlocutor_data["content"], 
        interlocutor_data["index"]
    )
    counter += 1  

    try:        
        full_result = generate_response(
            prompt, 
            previous_message
        )        
        result_text = full_result['data']['Get']\
                 [weaviate_class_name][0]\
                 ['_additional']['generate']\
                 ['groupedResult']
        response_code = 200
    except Exception as e:
        app.logger.error(
            f"An error occurred while getting data: {e}"
        )
        full_result = "Error"
        result_text = "Can you please repeat?"        
        response_code = 500
                        
    # save result_text to the session
    session[my_session_key] = result_text

    # text to speech using TTS models
    destination = os.path.join(
        "static/", 
        str(counter) + ".mp3"
    )
    text_to_speech(
        result_text, 
        voice,
        destination
    )
    

    return {
        "result_text": result_text,
        "result_audio": destination,
        "full_result": full_result,
        "interlocutor_data": interlocutor_data,     

    }, response_code

@app.route("/data", methods=["GET"])
def weaviate_get_all_data():
    try:
        result = weaviate_client.query.get(
            class_name=weaviate_class_name,
            properties=[
                "role",
                "content",
                "index"
            ]
        ).do()

        return {
            "data": result['data']['Get'][weaviate_class_name]
        }

    except Exception as e:
        app.logger.error(
            f"An error occurred while getting data: {e}"
        )
        return {
            "error in weaviate_data": str(e)
        }, 500 
EOT
```


```bash
docker-compose up --build
```


```bash
curl -X GET http://localhost:5000/init
```


```bash
curl -X GET http://localhost:5000/chat?role=assistant
```


```json
{
  "full_result": {
    "data": {
      "Get": {
        "Chat": [
          {
            "_additional": {
              "distance": 0.077032566,
              "generate": {
                "error": null,
                "groupedResult": "Absolutely, let's dive into the world of Joe Biden. Did you know that before he became the 46th President of the United States, Biden was a lifeguard at a pool in his hometown of Wilmington, Delaware? He claims that this job helped him to develop a sense of empathy and understanding towards racial discrimination. Isn't it fascinating how early life experiences can shape a person's future? What's your take on this?"
              }
            },
            "content": "Let's talk about this topic Joe Biden.",
            "index": 0,
            "role": "user"
          }
        ]
      }
    }
  },
  "interlocutor_data": {
    "content": "Let's talk about this topic Joe Biden.",
    "index": 0,
    "role": "user"
  },
  "result_audio": "static/1.mp3",
  "result_text": "Absolutely, let's dive into the world of Joe Biden. Did you know that before he became the 46th President of the United States, Biden was a lifeguard at a pool in his hometown of Wilmington, Delaware? He claims that this job helped him to develop a sense of empathy and understanding towards racial discrimination. Isn't it fascinating how early life experiences can shape a person's future? What's your take on this?"
}
```


```bash
curl -X GET http://localhost:5000/chat?role=user
```


## Using AI Avatar Models


```bash
mkvirtualenv -p python3.8 ai_avatar
```


```bash
git clone https://github.com/OpenTalker/SadTalker.git

cd SadTalker 

pip install \
    torch==1.12.1+cu113 \
    torchvision==0.13.1+cu113 \
    torchaudio==0.12.1 \
    --extra-index-url \
    https://download.pytorch.org/whl/cu113

pip install ffmpeg

pip install -r requirements.txt

bash scripts/download_models.sh
```


```bash
.
├── app
│   ├── app.py
│   ├── Dockerfile.app
│   ├── __pycache__
│   ├── requirements.txt
│   └── static
├── docker-compose.yaml
├── SadTalker
│   ├── app_sadtalker.py
│   ├── checkpoints
│   ├── cog.yaml
│   ├── docs
│   ├── examples
│   ├── gfpgan
│   ├── inference.py
│   ├── launcher.py
│   ├── LICENSE
│   ├── predict.py
│   ├── quick_demo.ipynb
│   ├── README.md
│   ├── req.txt
│   ├── requirements3d.txt
│   ├── requirements.txt
│   ├── results
│   ├── scripts
│   ├── src
│   ├── webui.bat
│   └── webui.sh
└── weaviate
    └── Dockerfile.weaviate
```


```bash
# User
cp SadTalker/examples/source_image/art_1.png \
app/static/user.png

# Assistant    
cp SadTalker/examples/source_image/art_2.png \
app/static/assistant.png
```


```bash
python \
SadTalker/inference.py \
--driven_audio ../app/static/0.mp3 \
--source_image ../app/static/user.png  \
--enhancer gfpgan
```


```bash
python \
SadTalker/inference.py \
--driven_audio ../app/static/1.mp3 \
--source_image ../app/static/assistant.png  \
--enhancer gfpgan \
--result_dir app/static
```


```bash
#!/bin/bash

# Loop for even numbers
for i in {0..10..2}; do
    python SadTalker/inference.py \
    --driven_audio app/static/$i.mp3 \
    --source_image app/static/user.png \
    --enhancer gfpgan \
    --result_dir app/static
done

# Loop for odd numbers
for i in {1..10..2}; do
    python SadTalker/inference.py \
    --driven_audio app/static/$i.mp3 \
    --source_image app/static/assistant.png \
    --enhancer gfpgan \
    --result_dir app/static
done
```


## What's Next?